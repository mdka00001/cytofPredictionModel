{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ee88e5b",
   "metadata": {},
   "source": [
    "# CyTOF Label Transfer: Advanced Workflow\n",
    "## Feature Evaluation, Selection, Custom Hyperparameters, and Prediction\n",
    "\n",
    "This notebook demonstrates the complete workflow for CyTOF label transfer with the new features:\n",
    "1. **Feature Evaluation**: Compute and visualize feature importance before training\n",
    "2. **Feature Selection**: Select features by group or importance threshold\n",
    "3. **Custom Hyperparameters**: Use custom XGBoost hyperparameter distributions\n",
    "4. **Model Training**: Train with cross-validated hyperparameter search\n",
    "5. **Evaluation**: Generate QC plots and metrics\n",
    "6. **Prediction**: Apply trained model to target timepoint\n",
    "\n",
    "**Prerequisites**:\n",
    "- conda environment from `environment.yml`\n",
    "- `.h5ad` file with timepoints and trusted labels for timepoints 1–4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7130ad59",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75969061",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'extract_xy' from 'cytof_label_transfer' (/home/md-adnan-karim/Documents/git_repo/cytofLabelTransfer/cytofPredictionModel/cytof_label_transfer/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# CyTOF Label Transfer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcytof_label_transfer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     17\u001b[39m     load_anndata,\n\u001b[32m     18\u001b[39m     split_timepoints,\n\u001b[32m     19\u001b[39m     extract_xy,\n\u001b[32m     20\u001b[39m     compute_feature_importance,\n\u001b[32m     21\u001b[39m     create_feature_groups,\n\u001b[32m     22\u001b[39m     plot_feature_importance,\n\u001b[32m     23\u001b[39m     select_features_by_groups,\n\u001b[32m     24\u001b[39m     select_features_by_importance,\n\u001b[32m     25\u001b[39m     select_features_interactive_report,\n\u001b[32m     26\u001b[39m     train_classifier,\n\u001b[32m     27\u001b[39m     load_hyperparameters_from_json,\n\u001b[32m     28\u001b[39m     predict_timepoint,\n\u001b[32m     29\u001b[39m )\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcytof_label_transfer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m extract_x_target\n\u001b[32m     31\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcytof_label_transfer\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mqc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m evaluate_and_plot_cv\n",
      "\u001b[31mImportError\u001b[39m: cannot import name 'extract_xy' from 'cytof_label_transfer' (/home/md-adnan-karim/Documents/git_repo/cytofLabelTransfer/cytofPredictionModel/cytof_label_transfer/__init__.py)"
     ]
    }
   ],
   "source": [
    "# Core imports\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# Data handling\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import anndata as ad\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# CyTOF Label Transfer\n",
    "from cytof_label_transfer import (\n",
    "    load_anndata,\n",
    "    split_timepoints,\n",
    "    extract_xy,\n",
    "    compute_feature_importance,\n",
    "    create_feature_groups,\n",
    "    plot_feature_importance,\n",
    "    select_features_by_groups,\n",
    "    select_features_by_importance,\n",
    "    select_features_interactive_report,\n",
    "    train_classifier,\n",
    "    load_hyperparameters_from_json,\n",
    "    predict_timepoint,\n",
    ")\n",
    "from cytof_label_transfer.data_utils import extract_x_target\n",
    "from cytof_label_transfer.qc import evaluate_and_plot_cv\n",
    "from cytof_label_transfer.model import TrainedModelBundle\n",
    "\n",
    "# Plotting configuration\n",
    "%matplotlib inline\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "\n",
    "print('✓ All libraries imported successfully')\n",
    "print(f'Working directory: {os.getcwd()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb6199e",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051bb836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure paths and parameters\n",
    "INPUT_H5AD = 'path/to/your_data.h5ad'  # UPDATE THIS PATH\n",
    "TIMEPOINT_COL = 'timepoint'             # Column name for timepoints\n",
    "LABEL_COL = 'celltype'                  # Column name for cell type labels\n",
    "OBSM_KEY = 'X_scVI_200_epoch'           # Optional: latent space key (set to None if not using)\n",
    "LAYER = None                            # Optional: layer name (None = use .X)\n",
    "\n",
    "# Output directories\n",
    "OUTPUT_DIR = Path('results')\n",
    "FEATURE_EVAL_DIR = OUTPUT_DIR / 'feature_evaluation'\n",
    "MODEL_DIR = OUTPUT_DIR / 'trained_model'\n",
    "\n",
    "# Training parameters\n",
    "TRAIN_TIMEPOINTS = [1, 2, 3, 4]  # Trusted timepoints\n",
    "TARGET_TIMEPOINT = 5              # Target timepoint for prediction\n",
    "\n",
    "# Create output directories\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "FEATURE_EVAL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODEL_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'✓ Configuration set')\n",
    "print(f'  Input file: {INPUT_H5AD}')\n",
    "print(f'  Output directory: {OUTPUT_DIR}')\n",
    "print(f'  Training timepoints: {TRAIN_TIMEPOINTS}')\n",
    "print(f'  Target timepoint: {TARGET_TIMEPOINT}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affebb86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AnnData object\n",
    "print(f'Loading data from {INPUT_H5AD}...')\n",
    "adata = load_anndata(INPUT_H5AD)\n",
    "\n",
    "print(f'\\n✓ Data loaded successfully')\n",
    "print(f'  Shape: {adata.n_obs} cells × {adata.n_vars} genes')\n",
    "print(f'  Timepoints: {sorted(adata.obs[TIMEPOINT_COL].unique())}')\n",
    "print(f'  Cell types: {adata.obs[LABEL_COL].nunique()} types')\n",
    "if OBSM_KEY:\n",
    "    print(f'  Latent space ({OBSM_KEY}): {adata.obsm[OBSM_KEY].shape[1]} dimensions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40211777",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into training and target\n",
    "print(f'Splitting data...')\n",
    "adata_train, adata_target = split_timepoints(\n",
    "    adata,\n",
    "    time_col=TIMEPOINT_COL,\n",
    "    train_timepoints=TRAIN_TIMEPOINTS,\n",
    "    target_timepoint=TARGET_TIMEPOINT,\n",
    ")\n",
    "\n",
    "print(f'\\n✓ Data split successfully')\n",
    "print(f'  Training set: {adata_train.n_obs} cells from timepoints {TRAIN_TIMEPOINTS}')\n",
    "print(f'  Target set: {adata_target.n_obs} cells from timepoint {TARGET_TIMEPOINT}')\n",
    "\n",
    "# Display cell type distribution in training set\n",
    "print(f'\\nCell type distribution in training set:')\n",
    "print(adata_train.obs[LABEL_COL].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cd3314",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for training\n",
    "print(f'Extracting features...')\n",
    "X_train, y_train, feature_names = extract_xy(\n",
    "    adata_train,\n",
    "    label_col=LABEL_COL,\n",
    "    use_layer=LAYER,\n",
    "    use_obsm_key=OBSM_KEY,\n",
    ")\n",
    "\n",
    "print(f'\\n✓ Features extracted')\n",
    "print(f'  Feature matrix shape: {X_train.shape}')\n",
    "print(f'  Total features: {len(feature_names)}')\n",
    "if OBSM_KEY:\n",
    "    n_markers = len(feature_names) - adata.obsm[OBSM_KEY].shape[1]\n",
    "    print(f'    - Markers: {n_markers}')\n",
    "    print(f'    - Latent ({OBSM_KEY}): {adata.obsm[OBSM_KEY].shape[1]}')\n",
    "print(f'  Class distribution: {dict(pd.Series(y_train).value_counts())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e541943",
   "metadata": {},
   "source": [
    "## 3. Feature Importance Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6b40f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute feature importance\n",
    "print('Computing feature importance (Random Forest)...')\n",
    "importances, _ = compute_feature_importance(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    feature_names,\n",
    "    method='random_forest',\n",
    "    n_estimators=100,\n",
    ")\n",
    "\n",
    "print(f'✓ Feature importance computed')\n",
    "print(f'\\nTop 10 most important features:')\n",
    "top_indices = np.argsort(importances)[::-1][:10]\n",
    "for rank, idx in enumerate(top_indices, 1):\n",
    "    print(f'  {rank:2d}. {feature_names[idx]:30s} importance={importances[idx]:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4645a982",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize feature importance\n",
    "print('Plotting feature importance...')\n",
    "plot_feature_importance(\n",
    "    importances,\n",
    "    feature_names,\n",
    "    top_n=30,\n",
    "    output_path=FEATURE_EVAL_DIR / 'feature_importance_top30.png',\n",
    "    figsize=(12, 8),\n",
    ")\n",
    "plt.show()\n",
    "print(f'✓ Plot saved to {FEATURE_EVAL_DIR / \"feature_importance_top30.png\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca4e324",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate feature importance report\n",
    "print('Generating feature importance report...')\n",
    "if OBSM_KEY:\n",
    "    feature_groups = create_feature_groups(feature_names, obsm_key=OBSM_KEY)\n",
    "else:\n",
    "    feature_groups = {'all_markers': list(range(len(feature_names)))}\n",
    "\n",
    "select_features_interactive_report(\n",
    "    importances,\n",
    "    feature_names,\n",
    "    feature_groups,\n",
    "    output_dir=FEATURE_EVAL_DIR,\n",
    ")\n",
    "print(f'✓ Report saved to {FEATURE_EVAL_DIR / \"feature_importance_report.csv\"}')\n",
    "\n",
    "# Load and display the report\n",
    "report_df = pd.read_csv(FEATURE_EVAL_DIR / 'feature_importance_report.csv')\n",
    "print(f'\\nFeature Importance Report (top 15):')\n",
    "print(report_df.head(15).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714b24de",
   "metadata": {},
   "source": [
    "## 4. Feature Selection (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44eb6692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Select by feature groups\n",
    "# Comment out if you want to use all features or Option 2\n",
    "\n",
    "FEATURE_SELECTION_MODE = 'all'  # Options: 'all', 'markers_only', 'latent_only', 'top_percentile', 'manual'\n",
    "\n",
    "selected_feature_indices = None\n",
    "selected_feature_names = None\n",
    "\n",
    "if FEATURE_SELECTION_MODE == 'markers_only' and OBSM_KEY:\n",
    "    print('Selecting markers only...')\n",
    "    selected_feature_indices, selected_feature_names = select_features_by_groups(\n",
    "        feature_names,\n",
    "        feature_groups,\n",
    "        ['markers'],\n",
    "    )\n",
    "    print(f'✓ Selected {len(selected_feature_indices)} marker features')\n",
    "\n",
    "elif FEATURE_SELECTION_MODE == 'latent_only' and OBSM_KEY:\n",
    "    print('Selecting latent features only...')\n",
    "    selected_feature_indices, selected_feature_names = select_features_by_groups(\n",
    "        feature_names,\n",
    "        feature_groups,\n",
    "        ['latent'],\n",
    "    )\n",
    "    print(f'✓ Selected {len(selected_feature_indices)} latent features')\n",
    "\n",
    "elif FEATURE_SELECTION_MODE == 'top_percentile':\n",
    "    print('Selecting features above 90th percentile...')\n",
    "    selected_feature_indices, selected_feature_names = select_features_by_importance(\n",
    "        importances,\n",
    "        feature_names,\n",
    "        percentile=90,\n",
    "    )\n",
    "    print(f'✓ Selected {len(selected_feature_indices)} features')\n",
    "\n",
    "elif FEATURE_SELECTION_MODE == 'manual':\n",
    "    # Manually specify feature indices (0-based)\n",
    "    print('Using manually selected features...')\n",
    "    # Example: top 20 features by importance\n",
    "    top_20_indices = np.argsort(importances)[::-1][:20]\n",
    "    selected_feature_indices = top_20_indices\n",
    "    selected_feature_names = [feature_names[i] for i in selected_feature_indices]\n",
    "    print(f'✓ Selected {len(selected_feature_indices)} features')\n",
    "\n",
    "else:\n",
    "    print('Using all features')\n",
    "\n",
    "if selected_feature_indices is not None:\n",
    "    print(f'\\nSelected features: {selected_feature_names}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e1c369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-extract features if selection was applied\n",
    "if selected_feature_indices is not None:\n",
    "    print('Re-extracting features with selection...')\n",
    "    X_train, y_train, feature_names = extract_xy(\n",
    "        adata_train,\n",
    "        label_col=LABEL_COL,\n",
    "        use_layer=LAYER,\n",
    "        use_obsm_key=OBSM_KEY,\n",
    "        selected_feature_indices=selected_feature_indices,\n",
    "    )\n",
    "    print(f'✓ Features re-extracted')\n",
    "    print(f'  Original features: {len(feature_names) + len(selected_feature_indices) - len(feature_names)}')\n",
    "    print(f'  Selected features: {len(feature_names)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e24f89",
   "metadata": {},
   "source": [
    "## 5. Custom Hyperparameters (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aaec271",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Use default hyperparameters\n",
    "USE_CUSTOM_HYPERPARAMS = False\n",
    "CUSTOM_HYPERPARAMS_FILE = 'custom_hyperparams.json'  # Path to JSON file\n",
    "\n",
    "param_distributions = None\n",
    "\n",
    "if USE_CUSTOM_HYPERPARAMS:\n",
    "    print(f'Loading custom hyperparameters from {CUSTOM_HYPERPARAMS_FILE}...')\n",
    "    param_distributions = load_hyperparameters_from_json(CUSTOM_HYPERPARAMS_FILE)\n",
    "    print(f'✓ Loaded {len(param_distributions)} hyperparameter settings')\n",
    "    print(f'\\nHyperparameter search space:')\n",
    "    for param, values in param_distributions.items():\n",
    "        print(f'  {param}: {values}')\n",
    "else:\n",
    "    print('Using default hyperparameter distributions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74084564",
   "metadata": {},
   "source": [
    "## 6. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3d4ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training configuration\n",
    "CV_FOLDS = 5\n",
    "CV_ITERATIONS = 30  # Number of random hyperparameter configurations to try\n",
    "USE_GPU = False     # Set to True if you have GPU and XGBoost GPU support\n",
    "\n",
    "print('='*60)\n",
    "print('STARTING MODEL TRAINING')\n",
    "print('='*60)\n",
    "print(f'Training set: {X_train.shape[0]} cells × {X_train.shape[1]} features')\n",
    "print(f'Number of classes: {len(np.unique(y_train))}')\n",
    "print(f'CV folds: {CV_FOLDS}')\n",
    "print(f'Hyperparameter iterations: {CV_ITERATIONS}')\n",
    "print(f'Using GPU: {USE_GPU}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ac97e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "bundle = train_classifier(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    feature_names=feature_names,\n",
    "    n_splits=CV_FOLDS,\n",
    "    n_iter=CV_ITERATIONS,\n",
    "    param_distributions=param_distributions,\n",
    "    output_dir=MODEL_DIR,\n",
    "    use_gpu=USE_GPU,\n",
    ")\n",
    "\n",
    "print(f'\\n✓ Model training completed')\n",
    "print(f'  Best CV F1 score: {bundle.cv_best_score:.4f}')\n",
    "print(f'  Number of classes: {len(bundle.label_names)}')\n",
    "print(f'  Number of features: {len(bundle.feature_names)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1cb0508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display training metrics\n",
    "metrics_file = MODEL_DIR / 'training_metrics.json'\n",
    "if metrics_file.exists():\n",
    "    with open(metrics_file) as f:\n",
    "        metrics = json.load(f)\n",
    "    \n",
    "    print('\\nTraining Metrics:')\n",
    "    print(f'  Cross-validated F1 (macro): {metrics[\"cv_best_score\"]:.4f}')\n",
    "    print(f'  Training F1 (macro): {metrics[\"train_f1_macro\"]:.4f}')\n",
    "    print(f'  Training Accuracy: {metrics[\"train_accuracy\"]:.4f}')\n",
    "    print(f'  Number of training samples: {metrics[\"n_samples\"]}')\n",
    "    print(f'  Number of features used: {metrics[\"n_features\"]}')\n",
    "    print(f'\\nBest hyperparameters:')\n",
    "    for param, value in metrics['best_params'].items():\n",
    "        print(f'  {param}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a48b496",
   "metadata": {},
   "source": [
    "## 7. Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfc3f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate QC plots and metrics\n",
    "print('Generating QC plots and evaluation metrics...')\n",
    "qc_dir = MODEL_DIR / 'qc'\n",
    "\n",
    "evaluate_and_plot_cv(\n",
    "    estimator=bundle.estimator,\n",
    "    X=X_train,\n",
    "    y=y_train,\n",
    "    class_names=bundle.label_names,\n",
    "    output_dir=qc_dir,\n",
    "    n_splits=CV_FOLDS,\n",
    "    label_encoder=bundle.label_encoder,\n",
    ")\n",
    "\n",
    "print(f'✓ QC plots saved to {qc_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f66ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display confusion matrix plot\n",
    "from matplotlib.image import imread\n",
    "\n",
    "cm_path = qc_dir / 'cv_confusion_matrix.png'\n",
    "if cm_path.exists():\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    img = imread(cm_path)\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    plt.title('Cross-validated Confusion Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print('Confusion matrix displayed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eda5a6c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display per-class F1 scores\n",
    "f1_path = qc_dir / 'cv_per_class_f1.png'\n",
    "if f1_path.exists():\n",
    "    fig, ax = plt.subplots(figsize=(12, 5))\n",
    "    img = imread(f1_path)\n",
    "    ax.imshow(img)\n",
    "    ax.axis('off')\n",
    "    plt.title('Per-class F1 Scores (Cross-validation)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print('Per-class F1 plot displayed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad1b7d7",
   "metadata": {},
   "source": [
    "## 8. Make Predictions on Target Timepoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d557168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features for target timepoint\n",
    "print(f'Extracting features for target timepoint {TARGET_TIMEPOINT}...')\n",
    "\n",
    "X_target, _ = extract_x_target(\n",
    "    adata_target,\n",
    "    use_layer=LAYER,\n",
    "    use_obsm_key=OBSM_KEY,\n",
    "    selected_feature_indices=selected_feature_indices,\n",
    ")\n",
    "\n",
    "print(f'✓ Target features extracted')\n",
    "print(f'  Shape: {X_target.shape}')\n",
    "print(f'  Number of cells: {X_target.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af98b6ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "print(f'Making predictions on {adata_target.n_obs} cells...')\n",
    "\n",
    "y_pred, y_proba = predict_timepoint(bundle, X_target)\n",
    "\n",
    "print(f'✓ Predictions completed')\n",
    "print(f'  Number of predictions: {len(y_pred)}')\n",
    "print(f'  Predicted classes: {np.unique(y_pred)}')\n",
    "print(f'\\nPrediction distribution:')\n",
    "print(pd.Series(y_pred).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0660c403",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract prediction confidence (max probability)\n",
    "if y_proba is not None:\n",
    "    max_confidence = y_proba.max(axis=1)\n",
    "    print(f'\\nPrediction Confidence Statistics:')\n",
    "    print(f'  Mean: {max_confidence.mean():.4f}')\n",
    "    print(f'  Median: {np.median(max_confidence):.4f}')\n",
    "    print(f'  Min: {max_confidence.min():.4f}')\n",
    "    print(f'  Max: {max_confidence.max():.4f}')\n",
    "    print(f'  Std: {max_confidence.std():.4f}')\n",
    "    \n",
    "    # Visualize confidence distribution\n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.hist(max_confidence, bins=30, edgecolor='black', alpha=0.7)\n",
    "    ax.axvline(max_confidence.mean(), color='red', linestyle='--', label=f'Mean: {max_confidence.mean():.3f}')\n",
    "    ax.set_xlabel('Prediction Confidence')\n",
    "    ax.set_ylabel('Number of Cells')\n",
    "    ax.set_title('Distribution of Prediction Confidence Scores')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d456983d",
   "metadata": {},
   "source": [
    "## 9. Write Predictions Back to AnnData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad82d0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add predictions to the full AnnData object\n",
    "print(f'Writing predictions to AnnData object...')\n",
    "\n",
    "# Initialize columns\n",
    "adata.obs['celltype_predicted'] = pd.Series(index=adata.obs_names, dtype='object')\n",
    "adata.obs['prediction_confidence'] = pd.Series(index=adata.obs_names, dtype='float')\n",
    "\n",
    "# Fill in predictions for target cells\n",
    "adata.obs.loc[adata_target.obs_names, 'celltype_predicted'] = y_pred\n",
    "\n",
    "if y_proba is not None:\n",
    "    adata.obs.loc[adata_target.obs_names, 'prediction_confidence'] = max_confidence\n",
    "\n",
    "print(f'✓ Predictions added to adata.obs')\n",
    "print(f'\\nColumns in adata.obs:')\n",
    "print(adata.obs[['celltype_predicted', 'prediction_confidence']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a20a9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save updated AnnData\n",
    "output_h5ad = OUTPUT_DIR / 'data_with_predictions.h5ad'\n",
    "\n",
    "print(f'Saving updated AnnData to {output_h5ad}...')\n",
    "adata.write_h5ad(output_h5ad)\n",
    "\n",
    "print(f'✓ Saved successfully')\n",
    "print(f'  File size: {output_h5ad.stat().st_size / 1024 / 1024:.1f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7b6d27",
   "metadata": {},
   "source": [
    "## 10. Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a392e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of original vs predicted labels (for target timepoint)\n",
    "if LABEL_COL in adata_target.obs.columns:\n",
    "    original_labels = adata_target.obs[LABEL_COL]\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'original': original_labels,\n",
    "        'predicted': y_pred,\n",
    "        'confidence': max_confidence if y_proba is not None else 1.0,\n",
    "    })\n",
    "    \n",
    "    print('Sample Predictions vs Original Labels:')\n",
    "    print(comparison_df.head(20).to_string())\n",
    "    \n",
    "    # Count matches\n",
    "    matches = (original_labels == y_pred).sum()\n",
    "    total = len(y_pred)\n",
    "    accuracy = matches / total\n",
    "    print(f'\\nComparison with original labels:')\n",
    "    print(f'  Matches: {matches}/{total} ({accuracy*100:.1f}%)')\n",
    "else:\n",
    "    print('Note: No original labels for target timepoint to compare')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e371f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print('\\n' + '='*60)\n",
    "print('WORKFLOW SUMMARY')\n",
    "print('='*60)\n",
    "print(f'\\nInput Data:')\n",
    "print(f'  Total cells: {adata.n_obs}')\n",
    "print(f'  Training set: {adata_train.n_obs} cells')\n",
    "print(f'  Target set: {adata_target.n_obs} cells')\n",
    "print(f'\\nFeatures:')\n",
    "print(f'  Initial features: {len(feature_names) + (len(selected_feature_indices) if selected_feature_indices is not None else 0)}')\n",
    "print(f'  Features used: {len(feature_names)}')\n",
    "print(f'\\nModel:')\n",
    "print(f'  Algorithm: XGBoost')\n",
    "print(f'  Classes: {len(bundle.label_names)}')\n",
    "print(f'  CV F1 Score: {bundle.cv_best_score:.4f}')\n",
    "print(f'\\nPredictions:')\n",
    "print(f'  Predictions made: {len(y_pred)}')\n",
    "if y_proba is not None:\n",
    "    print(f'  Mean confidence: {max_confidence.mean():.4f}')\n",
    "print(f'\\nOutput Files:')\n",
    "print(f'  Model: {MODEL_DIR}')\n",
    "print(f'  QC plots: {MODEL_DIR / \"qc\"}')\n",
    "print(f'  Predictions: {output_h5ad}')\n",
    "print('='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4e84f3",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "1. **Review the QC plots** in the `qc/` directory to assess model quality\n",
    "2. **Check the predictions** and their confidence scores\n",
    "3. **Validate results** by comparing with original labels if available\n",
    "4. **Adjust parameters** if needed:\n",
    "   - Try different feature selections\n",
    "   - Experiment with custom hyperparameters\n",
    "   - Increase/decrease CV iterations\n",
    "5. **Use the trained model** to predict on new data\n",
    "\n",
    "For more information, see:\n",
    "- [ADVANCED_USAGE.md](ADVANCED_USAGE.md) for detailed feature documentation\n",
    "- [PRACTICAL_EXAMPLES.md](PRACTICAL_EXAMPLES.md) for more code examples\n",
    "- [README.md](README.md) for installation and basic usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrna_spatial",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
